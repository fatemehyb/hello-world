{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatemehyb/hello-world/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKnGNqQJ67bG",
        "colab_type": "text"
      },
      "source": [
        "Enabling and testing GPU\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "Navigate to Editâ†’Notebook Settings\n",
        "select GPU from the Hardware Accelerator drop-down\n",
        "Next, we'll confirm that we can connect to the GPU with tensorflow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yqgafuda65PE",
        "colab_type": "code",
        "outputId": "832f8cdf-95f9-475e-b00a-964e01f2722f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx96b7w67M8-",
        "colab_type": "text"
      },
      "source": [
        "Observe TensorFlow speedup on GPU relative to CPU\n",
        "This example constructs a typical convolutional neural network layer over a random image and manually places the resulting ops on either the CPU or the GPU to compare execution speed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU8l686z7MK3",
        "colab_type": "code",
        "outputId": "6119ff13-9b46-465d-b63a-76f9e73b7c34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "with tf.device('/cpu:0'):\n",
        "  random_image_cpu = tf.random_normal((100, 100, 100, 3))\n",
        "  net_cpu = tf.layers.conv2d(random_image_cpu, 32, 7)\n",
        "  net_cpu = tf.reduce_sum(net_cpu)\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  random_image_gpu = tf.random_normal((100, 100, 100, 3))\n",
        "  net_gpu = tf.layers.conv2d(random_image_gpu, 32, 7)\n",
        "  net_gpu = tf.reduce_sum(net_gpu)\n",
        "\n",
        "sess = tf.Session(config=config)\n",
        "\n",
        "# Ensure that TF can detect the GPU.\n",
        "try:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "except tf.errors.InvalidArgumentError:\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise\n",
        "\n",
        "def cpu():\n",
        "  sess.run(net_cpu)\n",
        "  \n",
        "def gpu():\n",
        "  sess.run(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))\n",
        "\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "3.919786058999307\n",
            "GPU (s):\n",
            "0.18104511199999251\n",
            "GPU speedup over CPU: 21x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JuTiBhe7ewB",
        "colab_type": "code",
        "outputId": "78bc3012-ecec-453e-c675-81779d56fb3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import subprocess\n",
        "import sys\n",
        "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'SimpleITK'])\n",
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]    \n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)\n",
        "\n",
        "del_all_flags(tf.flags.FLAGS)\n",
        "# %run -i 'train.py' \n",
        "!python train.py \n",
        "# subprocess.check_call([sys.executable,'-h','python', 'train.py'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From train.py:142: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W1018 20:45:40.376641 140320919582592 module_wrapper.py:139] From train.py:142: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:83: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W1018 20:45:40.383282 140320919582592 module_wrapper.py:139] From train.py:83: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:154: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
            "\n",
            "W1018 20:45:40.393289 140320919582592 module_wrapper.py:139] From train.py:154: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
            "\n",
            "I1018 20:45:41.417953 140320919582592 utils.py:141] NumExpr defaulting to 2 threads.\n",
            "WARNING:tensorflow:From /content/NiftiDataset.py:44: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "W1018 20:45:42.161710 140320919582592 deprecation.py:323] From /content/NiftiDataset.py:44: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "WARNING:tensorflow:From train.py:206: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "W1018 20:45:42.191232 140320919582592 deprecation.py:323] From train.py:206: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "WARNING:tensorflow:From /content/VNet.py:117: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W1018 20:45:42.203469 140320919582592 module_wrapper.py:139] From /content/VNet.py:117: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/VNet.py:120: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "W1018 20:45:42.204844 140320919582592 deprecation.py:323] From /content/VNet.py:120: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/normalization.py:327: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "W1018 20:45:42.205659 140320919582592 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/normalization.py:327: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /content/Layers.py:94: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W1018 20:45:42.242430 140320919582592 module_wrapper.py:139] From /content/Layers.py:94: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/VNet.py:37: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W1018 20:45:42.291206 140320919582592 deprecation.py:506] From /content/VNet.py:37: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From train.py:250: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W1018 20:45:45.495827 140320919582592 module_wrapper.py:139] From train.py:250: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/dispatch.py:180: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use the `axis` argument instead\n",
            "W1018 20:45:45.524193 140320919582592 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/dispatch.py:180: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use the `axis` argument instead\n",
            "WARNING:tensorflow:From train.py:342: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
            "\n",
            "W1018 20:45:45.614850 140320919582592 module_wrapper.py:139] From train.py:342: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W1018 20:45:45.814256 140320919582592 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From train.py:373: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "W1018 20:45:49.694303 140320919582592 module_wrapper.py:139] From train.py:373: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "Setting up Saver...\n",
            "WARNING:tensorflow:From train.py:376: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W1018 20:45:49.695874 140320919582592 module_wrapper.py:139] From train.py:376: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:378: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W1018 20:45:49.978083 140320919582592 module_wrapper.py:139] From train.py:378: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:383: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "W1018 20:45:49.978428 140320919582592 module_wrapper.py:139] From train.py:383: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-10-18 20:45:49.984249: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-10-18 20:45:49.984749: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1e67800 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-18 20:45:49.984796: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-10-18 20:45:49.987881: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-10-18 20:45:50.043080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-18 20:45:50.043951: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1e679c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-18 20:45:50.043996: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-10-18 20:45:50.044231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-18 20:45:50.044971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-10-18 20:45:50.045374: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-18 20:45:50.046708: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-10-18 20:45:50.048058: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-10-18 20:45:50.048400: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-10-18 20:45:50.049951: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-10-18 20:45:50.051016: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-10-18 20:45:50.054647: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-10-18 20:45:50.054788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-18 20:45:50.055467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-18 20:45:50.056145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-10-18 20:45:50.056215: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-18 20:45:50.057726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-10-18 20:45:50.057759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-10-18 20:45:50.057774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-10-18 20:45:50.057950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-18 20:45:50.058621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-18 20:45:50.059285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9593 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From train.py:385: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "W1018 20:45:50.060265 140320919582592 module_wrapper.py:139] From train.py:385: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "2019-10-18 20:45:52.497851: Start training...\n",
            "WARNING:tensorflow:From train.py:389: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W1018 20:45:52.498173 140320919582592 module_wrapper.py:139] From train.py:389: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "2019-10-18 20:45:57.580948: Last checkpoint epoch: 0\n",
            "WARNING:tensorflow:From train.py:401: The name tf.train.global_step is deprecated. Please use tf.compat.v1.train.global_step instead.\n",
            "\n",
            "W1018 20:45:57.708666 140320919582592 module_wrapper.py:139] From train.py:401: The name tf.train.global_step is deprecated. Please use tf.compat.v1.train.global_step instead.\n",
            "\n",
            "2019-10-18 20:45:57.708445: Last checkpoint global step: 0\n",
            "tcmalloc: large alloc 8000004096 bytes == 0x7f9c6ccc4000 @  0x7f9f024db1e7 0x7f9efffa4d51 0x7f9f0000ecb6 0x7f9f0000f204 0x7f9f000a1d09 0x4f8925 0x4f98c7 0x4f7a28 0x4f876d 0x4f98c7 0x4f6128 0x4f7d60 0x4f876d 0x4f98c7 0x4f7a28 0x4f876d 0x4f98c7 0x4f6128 0x4f7d60 0x4f876d 0x4fa6c0 0x4f4944 0x4f876d 0x4f98c7 0x4f6128 0x4f9023 0x6415b2 0x64166a 0x643730 0x62b26e 0x4b4cb0\n",
            "2019-10-18 20:46:04.232842: Epoch 1 starts\n",
            "2019-10-18 20:46:15.305469: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-10-18 20:46:18.272450: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-10-18 20:46:56.894540: Saving checkpoint of epoch 1 at ./tmp/ckpt...\n",
            "2019-10-18 20:47:00.439691: Saving checkpoint succeed\n",
            "2019-10-18 20:47:00.450794: Training of epoch 1 finishes, testing start\n",
            "2019-10-18 20:47:13.207200: Epoch 2 starts\n",
            "2019-10-18 20:47:31.696609: Saving checkpoint of epoch 2 at ./tmp/ckpt...\n",
            "2019-10-18 20:47:34.141538: Saving checkpoint succeed\n",
            "2019-10-18 20:47:34.141661: Training of epoch 2 finishes, testing start\n",
            "2019-10-18 20:47:44.749245: Epoch 3 starts\n",
            "2019-10-18 20:48:03.198783: Saving checkpoint of epoch 3 at ./tmp/ckpt...\n",
            "2019-10-18 20:48:05.521391: Saving checkpoint succeed\n",
            "2019-10-18 20:48:05.521513: Training of epoch 3 finishes, testing start\n",
            "2019-10-18 20:48:16.349264: Epoch 4 starts\n",
            "2019-10-18 20:48:35.235798: Saving checkpoint of epoch 4 at ./tmp/ckpt...\n",
            "2019-10-18 20:48:37.724644: Saving checkpoint succeed\n",
            "2019-10-18 20:48:37.724818: Training of epoch 4 finishes, testing start\n",
            "2019-10-18 20:48:48.695789: Epoch 5 starts\n",
            "2019-10-18 20:49:07.443641: Saving checkpoint of epoch 5 at ./tmp/ckpt...\n",
            "2019-10-18 20:49:09.913630: Saving checkpoint succeed\n",
            "2019-10-18 20:49:09.913756: Training of epoch 5 finishes, testing start\n",
            "2019-10-18 20:49:21.066212: Epoch 6 starts\n",
            "2019-10-18 20:49:39.952434: Saving checkpoint of epoch 6 at ./tmp/ckpt...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "W1018 20:49:40.559543 140320919582592 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "2019-10-18 20:49:42.450008: Saving checkpoint succeed\n",
            "2019-10-18 20:49:42.450114: Training of epoch 6 finishes, testing start\n",
            "2019-10-18 20:49:53.438796: Epoch 7 starts\n",
            "2019-10-18 20:50:12.044017: Saving checkpoint of epoch 7 at ./tmp/ckpt...\n",
            "2019-10-18 20:50:14.471899: Saving checkpoint succeed\n",
            "2019-10-18 20:50:14.472031: Training of epoch 7 finishes, testing start\n",
            "2019-10-18 20:50:25.466862: Epoch 8 starts\n",
            "2019-10-18 20:50:44.146016: Saving checkpoint of epoch 8 at ./tmp/ckpt...\n",
            "2019-10-18 20:50:46.577356: Saving checkpoint succeed\n",
            "2019-10-18 20:50:46.577529: Training of epoch 8 finishes, testing start\n",
            "2019-10-18 20:50:57.104008: Epoch 9 starts\n",
            "2019-10-18 20:51:15.505004: Saving checkpoint of epoch 9 at ./tmp/ckpt...\n",
            "2019-10-18 20:51:17.879530: Saving checkpoint succeed\n",
            "2019-10-18 20:51:17.879660: Training of epoch 9 finishes, testing start\n",
            "2019-10-18 20:51:29.002813: Epoch 10 starts\n",
            "2019-10-18 20:51:47.559983: Saving checkpoint of epoch 10 at ./tmp/ckpt...\n",
            "2019-10-18 20:51:49.989231: Saving checkpoint succeed\n",
            "2019-10-18 20:51:49.989372: Training of epoch 10 finishes, testing start\n",
            "2019-10-18 20:52:00.995772: Epoch 11 starts\n",
            "2019-10-18 20:52:19.437492: Saving checkpoint of epoch 11 at ./tmp/ckpt...\n",
            "2019-10-18 20:52:21.841250: Saving checkpoint succeed\n",
            "2019-10-18 20:52:21.841376: Training of epoch 11 finishes, testing start\n",
            "2019-10-18 20:52:32.674076: Epoch 12 starts\n",
            "2019-10-18 20:52:51.170955: Saving checkpoint of epoch 12 at ./tmp/ckpt...\n",
            "2019-10-18 20:52:53.628374: Saving checkpoint succeed\n",
            "2019-10-18 20:52:53.628477: Training of epoch 12 finishes, testing start\n",
            "2019-10-18 20:53:04.510442: Epoch 13 starts\n",
            "2019-10-18 20:53:23.014155: Saving checkpoint of epoch 13 at ./tmp/ckpt...\n",
            "2019-10-18 20:53:25.392279: Saving checkpoint succeed\n",
            "2019-10-18 20:53:25.392383: Training of epoch 13 finishes, testing start\n",
            "2019-10-18 20:53:36.431631: Epoch 14 starts\n",
            "2019-10-18 20:53:55.046498: Saving checkpoint of epoch 14 at ./tmp/ckpt...\n",
            "2019-10-18 20:53:57.509763: Saving checkpoint succeed\n",
            "2019-10-18 20:53:57.509919: Training of epoch 14 finishes, testing start\n",
            "2019-10-18 20:54:08.333608: Epoch 15 starts\n",
            "2019-10-18 20:54:27.089776: Saving checkpoint of epoch 15 at ./tmp/ckpt...\n",
            "2019-10-18 20:54:29.479656: Saving checkpoint succeed\n",
            "2019-10-18 20:54:29.479794: Training of epoch 15 finishes, testing start\n",
            "2019-10-18 20:54:40.369179: Epoch 16 starts\n",
            "2019-10-18 20:54:59.381028: Saving checkpoint of epoch 16 at ./tmp/ckpt...\n",
            "2019-10-18 20:55:01.838355: Saving checkpoint succeed\n",
            "2019-10-18 20:55:01.838518: Training of epoch 16 finishes, testing start\n",
            "2019-10-18 20:55:12.412111: Epoch 17 starts\n",
            "2019-10-18 20:55:31.046707: Saving checkpoint of epoch 17 at ./tmp/ckpt...\n",
            "2019-10-18 20:55:33.435324: Saving checkpoint succeed\n",
            "2019-10-18 20:55:33.435463: Training of epoch 17 finishes, testing start\n",
            "2019-10-18 20:55:44.085967: Epoch 18 starts\n",
            "2019-10-18 20:56:02.476347: Saving checkpoint of epoch 18 at ./tmp/ckpt...\n",
            "2019-10-18 20:56:04.911701: Saving checkpoint succeed\n",
            "2019-10-18 20:56:04.911854: Training of epoch 18 finishes, testing start\n",
            "2019-10-18 20:56:15.419766: Epoch 19 starts\n",
            "2019-10-18 20:56:33.813908: Saving checkpoint of epoch 19 at ./tmp/ckpt...\n",
            "2019-10-18 20:56:36.223100: Saving checkpoint succeed\n",
            "2019-10-18 20:56:36.223233: Training of epoch 19 finishes, testing start\n",
            "2019-10-18 20:56:47.110210: Epoch 20 starts\n",
            "2019-10-18 20:57:05.791921: Saving checkpoint of epoch 20 at ./tmp/ckpt...\n",
            "2019-10-18 20:57:08.188401: Saving checkpoint succeed\n",
            "2019-10-18 20:57:08.188520: Training of epoch 20 finishes, testing start\n",
            "2019-10-18 20:57:19.157974: Epoch 21 starts\n",
            "2019-10-18 20:57:37.385228: Saving checkpoint of epoch 21 at ./tmp/ckpt...\n",
            "2019-10-18 20:57:39.807410: Saving checkpoint succeed\n",
            "2019-10-18 20:57:39.807566: Training of epoch 21 finishes, testing start\n",
            "2019-10-18 20:57:50.275399: Epoch 22 starts\n",
            "2019-10-18 20:58:08.879326: Saving checkpoint of epoch 22 at ./tmp/ckpt...\n",
            "2019-10-18 20:58:11.318449: Saving checkpoint succeed\n",
            "2019-10-18 20:58:11.318589: Training of epoch 22 finishes, testing start\n",
            "2019-10-18 20:58:21.926917: Epoch 23 starts\n",
            "2019-10-18 20:58:39.982510: Saving checkpoint of epoch 23 at ./tmp/ckpt...\n",
            "2019-10-18 20:58:42.346416: Saving checkpoint succeed\n",
            "2019-10-18 20:58:42.346550: Training of epoch 23 finishes, testing start\n",
            "2019-10-18 20:58:53.007251: Epoch 24 starts\n",
            "2019-10-18 20:59:11.193324: Saving checkpoint of epoch 24 at ./tmp/ckpt...\n",
            "2019-10-18 20:59:13.643974: Saving checkpoint succeed\n",
            "2019-10-18 20:59:13.644135: Training of epoch 24 finishes, testing start\n",
            "2019-10-18 20:59:24.060916: Epoch 25 starts\n",
            "2019-10-18 20:59:42.297309: Saving checkpoint of epoch 25 at ./tmp/ckpt...\n",
            "2019-10-18 20:59:44.687729: Saving checkpoint succeed\n",
            "2019-10-18 20:59:44.687917: Training of epoch 25 finishes, testing start\n",
            "2019-10-18 20:59:55.295358: Epoch 26 starts\n",
            "2019-10-18 21:00:13.976755: Saving checkpoint of epoch 26 at ./tmp/ckpt...\n",
            "2019-10-18 21:00:16.416509: Saving checkpoint succeed\n",
            "2019-10-18 21:00:16.416646: Training of epoch 26 finishes, testing start\n",
            "2019-10-18 21:00:27.169097: Epoch 27 starts\n",
            "2019-10-18 21:00:45.676343: Saving checkpoint of epoch 27 at ./tmp/ckpt...\n",
            "2019-10-18 21:00:48.097541: Saving checkpoint succeed\n",
            "2019-10-18 21:00:48.097711: Training of epoch 27 finishes, testing start\n",
            "2019-10-18 21:00:58.836588: Epoch 28 starts\n",
            "2019-10-18 21:01:17.536266: Saving checkpoint of epoch 28 at ./tmp/ckpt...\n",
            "2019-10-18 21:01:20.003486: Saving checkpoint succeed\n",
            "2019-10-18 21:01:20.003612: Training of epoch 28 finishes, testing start\n",
            "2019-10-18 21:01:30.434866: Epoch 29 starts\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}